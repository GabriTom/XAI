{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnHmYziaZVLC"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKWCgRtwZVLE"
      },
      "source": [
        "# XAI for image data with Captum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7df6bb8a"
      },
      "source": [
        "\n",
        "\n",
        "[Captum](https://captum.ai/) (\"comprehension\" in Latin) is an open\n",
        "source, extensible library for model interpretability built on PyTorch.\n",
        "\n",
        "With the increase in model complexity and the resulting lack of\n",
        "transparency, model interpretability methods have become increasingly\n",
        "important. Model understanding is both an active area of research as\n",
        "well as an area of focus for practical applications across industries\n",
        "using machine learning. Captum provides state-of-the-art algorithms,\n",
        "including Integrated Gradients, to provide researchers and developers\n",
        "with an easy way to understand which features are contributing to a\n",
        "model's output.\n",
        "\n",
        "Full documentation, an API reference, and a suite of tutorials on\n",
        "specific topics are available at the [captum.ai](https://captum.ai/)\n",
        "website.\n",
        "\n",
        "Introduction\n",
        "------------\n",
        "\n",
        "\n",
        "In this notebook, we'll look at Gradient-based methods, Perturbation methods and Local surrogate models.\n",
        "\n",
        "Let's recall the main characteristics of these broad categories:\n",
        "\n",
        "-   **Gradient-based algorithms** calculate the backward gradients of a\n",
        "    model output with respect to the input to find the features that mostly influenced the prediction. Some examples are **Vanilla Gradient**, **Grad CAM** and **Integrated Gradients**.\n",
        "-   **Perturbation-based algorithms** examine the changes in the output\n",
        "    of a model in response to changes in the input.\n",
        "    The input perturbations may be directed or random. **Occlusion,**\n",
        "    **Feature Ablation,** and **Feature Permutation** are all\n",
        "    perturbation-based algorithms available in Captum.\n",
        "-   **Local surrogate models** train an interpretable model in the neighborhood of the given sample to mimicking the behaviour of the original model. The explanation is then derived from the feature importance of the surrogate model. **LIME** is an example of surrogate model available in Captum.  \n",
        "\n",
        "\n",
        "We'll be examining algorithms of all types below.\n",
        "\n",
        "Captum also provide several easy-to-employ visualization tools for explanation algorithms. While we have seen that it is possible to create our\n",
        "own visualizations with Matplotlib, Captum offers enhanced tools specific to its attributions:\n",
        "\n",
        "-   The `captum.attr.visualization` module (imported below as `viz`)\n",
        "    provides helpful functions for visualizing attributions related to\n",
        "    images.\n",
        "-   **Captum Insights** is an easy-to-use API on top of Captum that\n",
        "    provides a visualization widget with ready-made visualizations for\n",
        "    image, text, and arbitrary model types.\n",
        "\n",
        "Both of these visualization toolsets will be employed in this\n",
        "notebook. We will focus on computer vision use\n",
        "cases, but the Captum can be actually employed on different kind of data, e.g., image, text and tabular as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To install Captum in an Anaconda or pip virtual environment, use the\n",
        "appropriate command for your environment below:\n",
        "\n",
        "With `conda`:\n",
        "\n",
        "``` {.sourceCode .sh}\n",
        "conda install pytorch torchvision captum flask-compress matplotlib -c pytorch\n",
        "```\n",
        "\n",
        "With `pip` (required in Colab):\n",
        "\n",
        "``` {.sourceCode .sh}\n",
        "pip install torch torchvision captum matplotlib Flask-Compress\n",
        "```\n"
      ],
      "metadata": {
        "id": "Zgl0vHMExyck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision captum matplotlib Flask-Compress"
      ],
      "metadata": {
        "id": "QBTTZj4uxtpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see from above, after executing this cell the first time you need to **restart the session** to load the installed libraries"
      ],
      "metadata": {
        "id": "NpQlnz0KyMMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A First Example\n",
        "---------------\n",
        "\n",
        "To start, let's take a simple example. We'll again employ a ResNet\n",
        "model pretrained on the ImageNet dataset. We'll get a test input, and\n",
        "use the simple Vanilla Gradient algorithm that we previously implemented to examine how Captum works.  We will also employ Captum visualization tool to see a helpful visualization of this input attribution map for some test images.\n",
        "\n",
        "First, some imports:\n"
      ],
      "metadata": {
        "id": "B88gHV3px7S_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4UWh1esZVLG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import captum\n",
        "from captum.attr import Occlusion, LayerGradCam, Saliency\n",
        "from captum.attr import visualization as viz\n",
        "\n",
        "import os, sys\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwqvTdidZVLG"
      },
      "source": [
        "Now we'll use the TorchVision model library to download a pretrained\n",
        "ResNet. Since we're not training, we'll place it in evaluation mode for\n",
        "now.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X94j4eVeZVLG",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model = models.resnet18(weights=models.ResNet50_Weights)\n",
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7meqZ6qZVLG"
      },
      "source": [
        "In the lab4-b.zip file from where you extract this notebook should also have some files (e.g., `cat.jpg`) in it. Place all the files in the same place where you are executing this notebook (or upload them if you are using Colab).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGqD2PLXZVLH"
      },
      "outputs": [],
      "source": [
        "test_img = Image.open('cat.jpg')\n",
        "test_img_data = np.asarray(test_img)\n",
        "plt.imshow(test_img_data)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98p6bFRbZVLH"
      },
      "source": [
        "Our ResNet model was trained on the ImageNet dataset, and expects images\n",
        "to be of a certain size, with the channel data normalized to a specific\n",
        "range of values. We'll also pull in the list of human-readable labels\n",
        "for the categories our model recognizes - it should be among the unzipped files as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLNgaOfaZVLH"
      },
      "outputs": [],
      "source": [
        "# model expects 224x224 3-color image\n",
        "transform = transforms.Compose([\n",
        " transforms.Resize(224),\n",
        " transforms.CenterCrop(224),\n",
        " transforms.ToTensor(),\n",
        "\n",
        "])\n",
        "norm_transform = transforms.Normalize(\n",
        "     mean=[0.485, 0.456, 0.406],\n",
        "     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "transformed_img = transform(test_img)\n",
        "input_img = norm_transform(transformed_img)\n",
        "input_img = input_img.unsqueeze(0) # the model requires a dummy batch dimension\n",
        "\n",
        "# we restore the transformed img to be a standard numpy array\n",
        "orig_img = np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0))\n",
        "\n",
        "labels_path = 'imagenet_class_index.json'\n",
        "with open(labels_path) as json_data:\n",
        "    idx_to_labels = json.load(json_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxrHnrlSZVLH"
      },
      "source": [
        "Now, we can ask the question: What does our model think this image\n",
        "represents?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ECluKMHZVLH"
      },
      "outputs": [],
      "source": [
        "output = model(input_img)\n",
        "output = F.softmax(output, dim=1)\n",
        "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
        "pred_label_idx.squeeze_()\n",
        "predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
        "print('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yueGxubQZVLI"
      },
      "source": [
        "We've confirmed that ResNet thinks our image of a cat is, in fact, a\n",
        "cat (a `tabby` cat, in particular, which is the most common type of domestic cat) . But *why* does the model think this is an image of a cat?\n",
        "\n",
        "For the answer to that, we turn to Captum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXT5Ah_VZVLI"
      },
      "source": [
        "Gradient-based methods: Vanilla Gradient\n",
        "=============================================\n",
        "\n",
        "Saliency methods use a specific input - here, our test image - to generate\n",
        "a map of the relative importance. In particular they all return a matrix $S \\in \\mathbb{R}^{H\\times W}$ representing the positive importance of each input pixel to a given output class.\n",
        "\n",
        "\n",
        "[Saliency](https://captum.ai/api/saliency.html) (i.e., Vanilla Gradient)\n",
        "is one of the feature attribution algorithms available in Captum. However, most of them behave similarly (by calling the method `.attribute(input_img, target)`), with dedicated parameters to be tuned in some cases.\n",
        "\n",
        "Once we have the importance map from Saliency, we'll use the\n",
        "visualization tools in Captum to represent the\n",
        "importance map. Captum's `visualize_image_attr()` function provides a\n",
        "variety of options for customizing display of your attribution data.\n",
        "Here we use it to plot both the original image and the attribution map. Note that the latter has to be converted to a numpy array following the standard image format $H\\times W \\times C$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPm4rVhEZVLI"
      },
      "outputs": [],
      "source": [
        "# Initialize the attribution algorithm with the model\n",
        "vanilla_gradient = Saliency(model)\n",
        "\n",
        "# Ask the algorithm to attribute our output target to the most important input pixels\n",
        "attributions_vg = vanilla_gradient.attribute(input_img, target=pred_label_idx)\n",
        "\n",
        "# Show the original image for comparison\n",
        "_ = viz.visualize_image_attr(None, orig_img,\n",
        "                      method=\"original_image\", title=\"Original Image\")\n",
        "\n",
        "# Show vanilla-gradient\n",
        "_ = viz.visualize_image_attr(np.transpose(attributions_vg.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                             orig_img,\n",
        "                             method=\"heat_map\", title=\"Vanilla Gradient\",\n",
        "                             cmap=matplotlib.cm.get_cmap('plasma'),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcw_U1ynZVLI"
      },
      "source": [
        "In the image above, you should see that Vanilla Gradients gives us\n",
        "most of the signal around the cat's location in the image, particularly around the muzzle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's try other visualization methods!\n",
        "\n",
        "Captum provides several ways to visualize Saliency maps.\n",
        "`viz.vizualize_image_attr` provides visual attribution for a given image by normalizing attribution values of the desired sign (positive, negative, absolute value, or all) and displaying them using the desired mode in a matplotlib figure.\n",
        "\n",
        "The `method` argument is of particular importance as it allows to create nice overlaying of the attribution matrix with the original image.\n",
        "\n",
        "    def visualize_image_attr(attr: ndarray, original_image: Union[None, ndarray]=None, method: str='heat_map',\n",
        "        sign: str='absolute_value', ...)\n",
        "\n",
        "    method (str, optional): Chosen method for visualizing attribution.\n",
        "                Supported options are:\n",
        "            1. `heat_map` - Display heat map of chosen attributions\n",
        "            2. `blended_heat_map` - Overlay heat map over greyscale  \n",
        "                   version of original image. Parameter alpha_overlay\n",
        "                corresponds to alpha of heat map.\n",
        "            3. `original_image` - Only display original image.\n",
        "            4. `masked_image` - Mask image (pixel-wise multiply)  \n",
        "                   by normalized attribution values.\n",
        "            5. `alpha_scaling` - Sets alpha channel of each pixel  \n",
        "                   to be equal to normalized attribution value.\n",
        "                Default: heat_map\n",
        "\n",
        "In the following, provides 3 other visualizations employing\n",
        " * `masked_image`\n",
        " * `blended_heat_map`\n",
        " * `alpha scaling`\n",
        "\n",
        "*Note*: For `blended_heat_map` employ the color map: `cmap=matplotlib.cm.get_cmap('plasma')` and increase the `alpha_overlay`. Always use `sign='positive'`.\n",
        "\n",
        "Which one do you like the most?"
      ],
      "metadata": {
        "id": "VqrTy_cZ_qUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### COMPLETE ~15 lines expected ###\n"
      ],
      "metadata": {
        "id": "CeonXAEw_yul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "viz.visualize_image_attr_multiple\n",
        "--------------------------------\n",
        "\n",
        "This is a variant of the previous visualization method, in which the parameters `signs`, `methods`  and `titles` support also a list to plot many visualization at the same time. Try to do all previous plots in a single one, passing to the previous function the following arguments:\n",
        "\n",
        "*   `methods=[\"original_image\", \"masked_image\", \"blended_heat_map\", \"alpha_scaling\"]`\n",
        "*   `signs=[\"all\", \"positive\", \"positive\", \"positive\"]`\n",
        "*   `titles=[\"Original\", \"Image masking\", \"Blended heat map\", \"Alpha scaling\"]`\n",
        "\n",
        "\n",
        "As you can notice we also require to plot the original image for comparison.\n"
      ],
      "metadata": {
        "id": "qNxcWUfA0Kco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### COMPLETE ~ 6 lines expected ###\n"
      ],
      "metadata": {
        "id": "Xnb_ODXz0KBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Ats9l2ZVLI"
      },
      "source": [
        "Perturbation-based methods\n",
        "==================================\n",
        "\n",
        "Gradient-based attribution methods compute the output changes with respect to the input by backpropagation of the output loss.\n",
        "*Perturbation-based attribution* methods, instead, take a direct approach by\n",
        "introducing changes to the input and actually measuring the effect on the output.\n",
        "\n",
        "[Occlusion](https://captum.ai/api/occlusion.html)[1], in particular,\n",
        "performs the following steps:\n",
        "* It selects a patch $W\\in R^{W_w\\times H_w \\times 3}$ initialized to $[0.5, 0.5, 0.5]$ (gray color)\n",
        "* It subsequently replace sections of the input image with this patch\n",
        "* It examine the effect on the output signal:\n",
        "    - A prediction score decrease implies that masked pixels were *positively* important\n",
        "    - A prediction score increase implies that masked pixels were *negatively* important\n",
        "\n",
        "As you may guess, it is computationally more expensive.\n",
        "\n",
        "<small>[1] Zeiler, M. D., & Fergus, R. (2014). Visualizing and understanding convolutional networks. In Computer Vision-ECCV 2014: 13th European Conference.</small>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Below, you have to try the Occlusion attribution. Similarly to configuring a\n",
        "convolutional neural network, you must specify:\n",
        "* the sliding_window_shapes=(3,W_w, H_w) (e.g., (3,15,15))\n",
        "* stride length to determine the spacing of individual measurements (e.g., stride=(3, 8, 8) .\n",
        "\n",
        "Again we will visualize the output of our Occlusion attribution\n",
        "with `visualize_image_attr_multiple()`, showing heat maps of both\n",
        "positive and negative attribution by region, and by masking the original\n",
        "image with the positive attribution regions. The masking gives a very\n",
        "instructive view of what regions of our cat photo the model found to be\n",
        "most \"cat-like\".\n"
      ],
      "metadata": {
        "id": "vc-ll6Xr72e1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqH8K4x5ZVLI"
      },
      "outputs": [],
      "source": [
        "occlusion = Occlusion(model)\n",
        "\n",
        "### COMPLETE ~ 5 lines expected ###\n",
        "\n",
        "\n",
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                                      orig_img,\n",
        "                                      [\"original_image\", \"blended_heat_map\", \"blended_heat_map\", \"masked_image\"],\n",
        "                                      [\"all\", \"positive\", \"negative\", \"positive\"],\n",
        "                                      titles=[\"Original\", \"Positive Attribution\", \"Negative Attribution\", \"Masked\"],\n",
        "                                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJIJIQ_EZVLI"
      },
      "source": [
        "Again, we see greater significance placed on the region of the image\n",
        "that contains the cat, this time particularly around the ears. The muzzle (previously important) seems to be negatively important with Occlusion.\n",
        "\n",
        "This is most likely due to the fact that the Occlusion method is an approximation and that we are using a big patch. Try to reduce the patch dimension as well as the stride to get better results.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArqC15sKZVLI"
      },
      "source": [
        "GradCAM\n",
        "---------------------------------\n",
        "\n",
        "GradCAM computes the gradients of the target output with respect to the\n",
        "given layer (normally the last convolutional layer), it averages for each output channel,\n",
        "and multiplies the average gradient for each channel by the layer\n",
        "activations.\n",
        "GradCAM is designed for convnets; since the activity of convolutional layers often maps spatially to the input, GradCAM attributions are often upsampled\n",
        "and used to mask the input.\n",
        "\n",
        "For [`LayerGradCam`](https://captum.ai/api/layer.html#gradcam) (the name of GradCAM in Captum), in addition to the model, you must specify a hidden layer within the model that you wish to examine. Check the following model summary to find the name of the last convolutional layer.\n",
        "\n",
        "As previously, when using `.attribute()`, recall to specify the target class of interest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "9EdKaHNv_xos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjT4QbT0ZVLI"
      },
      "outputs": [],
      "source": [
        "### COMPLETE ~ 2 lines expected ###\n",
        "\n",
        "\n",
        "_ = viz.visualize_image_attr(attributions_lgc[0].cpu().permute(1,2,0).detach().numpy(),\n",
        "                             sign=\"all\",\n",
        "                             title=\"Activation of the last conv layer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK5fkATRZVLJ"
      },
      "source": [
        "We'll use the convenience method `interpolate()` of\n",
        "`LayerGradCam` to upsample this attribution data, to compare it with the original\n",
        "image. As previously, plot the original image, the blended heatmap of the positive and negative attributions, and the masked image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1XwGZsUZVLJ"
      },
      "outputs": [],
      "source": [
        "upsamp_attr_lgc = LayerGradCam.interpolate(attributions_lgc, input_img.shape[2:])\n",
        "\n",
        "print(attributions_lgc.shape)\n",
        "print(upsamp_attr_lgc.shape)\n",
        "print(input_img.shape)\n",
        "\n",
        "\n",
        "### COMPLETE ~5 lines expected ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR TURN!\n",
        "=========\n",
        "\n",
        "Now it is your turn to write everything from scratch. You have to provide visualization for the following methods:\n",
        "*   [Input$\\times$Gradient](https://captum.ai/api/input_x_gradient.html)\n",
        "*   [Integrated Gradient](https://captum.ai/docs/extension/integrated_gradients)\n",
        "*   [LIME](https://captum.ai/api/lime.html)\n",
        "\n",
        "For the first two methods check whether using SmoothGrad (a.k.a. [NoiseTunnel](https://captum.ai/api/noise_tunnel.html) in Captum) improves the visualization.\n",
        "\n",
        "Optionally, visualize also the explanations for the other images provided.\n",
        "\n",
        "*Note* : Since Input$\\times$Gradient methods already combine the gradient with the input, don't plot the attribution over the image (\"blended_heat_map\") but rather only the positive attribution with the \"heat_map\" method and the masked input.\n",
        "\n",
        "\n",
        "*Note2* : In LIME, the attribution function requires the superpixels to train of the intepretable function (parameter `feature_mask` of LIME `attr` method). To get the superpixels masks you can use the following snippet. You will need to play with the parameter of both the `slic` function and the `attribute` function to extract a nice visualization.\n",
        "\n",
        "\n",
        "```\n",
        "from skimage.segmentation import slic, mark_boundaries\n",
        "\n",
        "# extracting super pixels\n",
        "feature_mask = slic(orig_img)\n",
        "\n",
        "# map segment IDs to feature group IDs\n",
        "fig = plt.figure(\"Superpixels\")\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.imshow(mark_boundaries(orig_img, feature_mask))\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print('Feature mask IDs:', np.unique(feature_mask).tolist())\n",
        "print('Feature mask', feature_mask)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WJYM1IEGG_uB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input x Gradient\n",
        "\n"
      ],
      "metadata": {
        "id": "k5hI7ViI3i21"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KEfz6CWoap-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With SmoothGrad"
      ],
      "metadata": {
        "id": "8I5KDhsRJbtN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0cammVhnaqey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrated Gradients\n"
      ],
      "metadata": {
        "id": "W9HuG2yaHJ-w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61UmJkqIasIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With SmoothGrad\n",
        "\n"
      ],
      "metadata": {
        "id": "PNjrAORgMBlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "viD4lcoJatwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIME"
      ],
      "metadata": {
        "id": "1pWSWHFNUd09"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7tUCigO9awj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}